\chapter{LITERATURE STUDY}
\label{chap:literaturestudy}

% Ubah bagian-bagian berikut dengan isi dari tinjauan pustaka

This chapter will explain the supporting theories that are used as references for this study.
The theories described in this chapter will be presented in a systematic order, starting from the most basic
to deeper explanations.

\section{Pose Estimation}
\label{sec:poseestimation}

% Contoh input gambar
\begin{figure}[ht]
  \centering

  % Ubah dengan nama file gambar dan ukuran yang akan digunakan
  \includegraphics[scale=1]{gambar/taxonomy-pose-estimation.png}

  % Ubah dengan keterangan gambar yang diinginkan
  \caption{Taxonomy of pose estimation approaches}
  \label{fig:pose-estimation}
\end{figure}

Pose estimation is a heavily explored area with applications in gaming, animation, action recognition, activity tracking, and augmented reality.
In order to improve pose estimate outcomes, various approaches have been developed. These methods may generally be split into: Single-person and Multi-person approaches, 
as depicted in figure \ref{fig:pose-estimation}. The single-person approach is fundamentally a regression issue because it just determines the pose of a single person in an image, 
the person's position and an implicit number of keypoints are already known. However, the multi-person approach tries to solve an unconstrained problem because we do not know 
the positions and number of persons within the image \parencite{romeo}.

The single-person approach is divided into two frameworks based on the keypoint prediction method: directly regressing keypoints from the features (i.e. direct regression based framework)
or by generating heatmaps and inferencing keypoints via heatmap (i.e. heatmap based framework) \parencite{romeo}.
A direct regression-based framework can be implemented in various ways: as done by \parencite{toshev2014}, 
they presented DeepPose where their model uses a simple architecture with a convolutional layer, followed by a dense layer that will produce keypoint values in \emph{(x,y)}.
Other authors \parencite{carreira2015} suggested a technique that iteratively improves model output by feeding back mistake predictions, leading to a notable improvement in accuracy.

Then for a heatmap based framework, an alternative method can be used to generate heatmaps of all keypoints in the image
rather than directly predicting them. The final stick figure is then created using additional techniques to know the connection between keypoints or joints.
In \parencite{chen2014}, authors proposed a graphical model with pairwise
relations to make adaptive use of local image measurements. Later on, both the detection of joints and the prediction of their relationships can be accomplished using those local image measurements.
\parencite{newell2016} designed a \emph{"stacked hourglass"} network, that is closely similar to encoder-decoder architecture and is based on the sequential phases of pooling and upsampling.
They demonstrated the importance of repeated bottom-up, top-down processing with intermediate supervision for enhancing the effectiveness of human pose detection.

The multi-person approach is a more complex task because
the positions and number of persons within the image are unknown, therefore the framework has to detect keypoints and assemble an unknown number of persons. To overcome this task,
two pipelines have been proposed: top-down pipeline and bottom-up pipeline \parencite{romeo}.
Beginning with the detection of every person present in an image, the top-down pipeline creates bounding boxes. The following action uses each of the identified bounding boxes and applies a single-person method. 
For each person that is discovered, the single-person technique will generate keypoints, and then, as shown in Figure \ref{fig:top-down-approach}, the pipeline may include extra steps of post-processing and improving final results.

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.8]{gambar/top-down-approach.png}
  \caption{The top-down pipeline in multi-person approach for pose estimation}
  \label{fig:top-down-approach}
\end{figure}

Compare to a top-down pipeline, the bottom-up pipeline operates in reverse. The bottom-up pipeline begins by finding all of the keypoints, which are then connected to human instances, as seen in Figure \ref{fig:bottom-up-approach}. 
The bottom-up pipeline is probably quicker than the top-down pipeline because it doesn't detect human bounding boxes and runs pose estimation for each person individually.

\begin{figure}[ht]
  \centering
  \includegraphics[scale=1.2]{gambar/bottom-up-approach.png}
  \caption{The bottom-up pipeline in multi-person approach for pose estimation.}
  \label{fig:bottom-up-approach}
\end{figure}

\section{Human Pose Estimation}
\label{sec:humanposeestimation}

Estimating human poses is one of the most challenging things in computer vision, which aims to determine the position or spatial location of certain points of a person's body (body parts/joints) from a given image or video.
Essentially it is a way to capture a set of coordinates by defining the human body joints like wrist, shoulder, knees, eyes, ears, ankles, and arms, which is a key point in images and videos that can describe a pose of a person. Then, when an image or video is given to the pose estimator model as input, it identifies the coordinates of these detected body parts and joints as output and a confidence score showing the precision of the estimations.
For many years, the main topic of discussion for numerous classical object detection applications has been the detection of persons.
With recent developments in machine-learning algorithms, computers can now understand human body language by performing pose detection and pose tracking. It has now reached a point where the hardware requirements to operate and the detection accuracy make them commercially viable.
Several industries, including security, business intelligence, health and safety, and entertainment, will be profoundly impacted by human pose estimate. Autonomous driving is one such application where this method has already shown its viability. Computers can sense and predict pedestrian behavior far more comprehensively with the use of real-time human pose detection and tracking, allowing for more consistent driving.

Pose estimation on humans can be divided into two techniques: 2D Pose Estimation and 3D Pose Estimation.
2D pose estimation is a type of pose estimation that can estimate the locations of the body joints in 2D space relative to input data (i.e., image or video frame). The location is represented with X and Y coordinates for each key point. On the other hand, 3D pose estimation transforms a 2D image into a 3D object by estimating an additional Z-dimension to the prediction. 3D pose estimation enables us to predict the accurate spatial positioning of a represented person or thing.

In the early days, the pose estimation job was treated as a part-based inference task, and there were two general groups of models. The first one is called appearance models, where features of body parts are first extracted by feature descriptors like Histogram of Oriented Gradient then distinct body parts are combined together.
Another group of models is deformable models or structural models. 
The performance of estimation models rapidly increases after deep convolutional networks are used for pose estimation. Researchers initially concentrate primarily on a well-cropped single-person posture estimate problem, which is a simplified sub-task. These days, more difficult situations, including pose estimation in a crowd, have become hot subjects thanks to the success of the more general multi-person pose estimation \parencite{song2021}.

\subsection{YOLO-pose}
\label{subsec:yolopose}

YOLO-pose is a single-shot approach like other bottom-up approaches. However, it does not use heatmaps. Rather, it associates all keypoints of a person with anchors. Figure \ref{fig:YOLO-pose-architecture} illustrates the overall architecture with keypoint
heads for pose estimation. The most advanced detector human in terms of complexity and accuracy is YOLOv5. Consequently, they choose it as their foundation and build upon it.
The primary focus of YOLOv5 is the recognition of 80 class COCO objects, with the box head predicting 85 elements per anchor. There are three anchors of various shapes for each grid location \parencite{maji2022yolopose}.

\begin{figure}[ht]
  \centering
  \includegraphics[scale=1.01]{gambar/yolo-architecture.png}
  \caption{YOLO-pose architecture.}
  \label{fig:YOLO-pose-architecture}
\end{figure}

For human pose estimation, it comes down to a single-class person detection problem with each person having 17 associated keypoints, each of which is again identified with a location and confidence: \emph{{x, y, conf}}.
Therefore, there are a total of 51 elements for the 17 keypoints associated with an anchor. As a result, the keypoint head predicts 51 elements and the box head predicts six elements for each anchor.
For an anchor with n keypoints, the overall prediction vector is defined as:
\begin{equation}
  \label{eq:yoloresult}
  P_v = \bigl\{ C_x, C_y, W, H, box_{conf}, class_{conf}, K_x^1, K_y^1, K_{conf}^1, ..., ..., ..., K_x^n, K_y^n, K_{conf}^n \bigr\}
\end{equation}
Keypoint confidence is trained based on the keypoint's visibility flag. Ground truth confidence is assigned to one if a keypoint is either visible or occluded. Otherwise, if it is outside of the field of view, confidence is set to zero.
YOLO-Pose uses CSP-darknet53 \parencite{wang2020} as backbone and PANet \parencite{liu2018} for fusing features of various scales from the backbone.
The Input image is passed through darknetcsp backbone that generates feature maps at
various scales \{P3, P4, P5, P6\}. PAnet is used for fusing these feature maps across multiple scales and the output is fed to detection
heads. Finally each detection head branches into box head and keypoint head \parencite{maji2022yolopose}.


\section{Humanoid Robot Pose Estimation}
\label{sec:humanoidrobotposeestimation}

Humanoid robots and people have similar shapes, which has both advantages and disadvantages. On the one hand, this allows us to start with approaches already in use for people, but on the other, it makes it harder for us to distinguish between people and humanoid robots \parencite{amini2021}.
As explained in Section \ref{sec:poseestimation}, in general, the effort to tackle the problem of pose estimation is vary depend on the number of persons (single-person or multi-person), for multiple persons can be categorized as a top-down or bottom-up approach.
In the top-down approach, the initial stage is to identify specific individuals, and the subsequent step is to implement pose estimation. The fact that the model's performance is closely associated with the performance of the person detector is one of the drawbacks of this approach. Although the state-of-the-art (SOTA) results are derived from this type of approach,  the runtime of such approaches is negatively affected by the number of persons present,
as a single-person pose estimator is run for each detection. Since the computational cost grows linearly as the number of users increases, performance is frequently not real-time \parencite{amini2021}.

Bottom-up techniques, on the other hand, are less reliant on the number of people in the image because they simultaneously identify body joints and classify them into individuals. Accurately grouping the detected keypoints in real-time is one of the bottom-up method's fundamental issues.
Recent methods arrange the identified keypoints into distinct instances using a greedy algorithm. In addition, compared to the top-down method, the bottom-up method's effectiveness is more influenced by the various scales of the people in a given image. Previous research has relied on high-resolution input size \parencite{papandreou2018} or the scale search method \parencite{cao2019} to address this issue. However, the inference time is growing as a result of these strategies.
A time-efficient method predicting keypoints at higher resolution was introduced by Cheng et al. \parencite{cheng2020}, narrowing the performance gap between bottom-up and top-down models.

There are three Pose Estimation Models for humanoid robots that we retrained with the
new dataset. Two of them use a bottom-up approach and one uses a top-down approach. For a detailed explanation of each model is located in following section.

\subsection{NimbRo's Model}
\label{subsec:nimbromodel}

NimbRo's model chose to use a similar architecture with NimbRo-Net and NimbRo-Net2 because it had such positive outcomes.
Their model, an encoder-decoder network, takes an RGB image with dimensions of \emph{w x h}. Rather than use the decoder to create a powerful feature extractor, they observe it is more required to use a deeper encoder.
A pre-trained ResNet model \parencite{he2016} is used as the encoder, and the last fully connected and global average pooling layers have been removed.
The first layer is a 7 x 7 convolutional with stride 2, followed by a max-pooling layer.  The remaining four modules of the encoder network are residual blocks, which have lower resolutions and higher depths as the number of modules rises.
Depending on the ResNet architecture that was chosen, each residual block is made up of two or three convolutional layers, followed by batch normalization, ReLU activation, and a shortcut connection. Early layers have more fine-grained spatial information,
while final layers contain more semantic information that network has extracted \parencite{amini2021}.

\begin{figure}[ht]
  \centering
  \includegraphics[scale=1]{gambar/nimbro-architecture.png}
  \caption{NimbRo's model architecture}
  \label{fig:nimbro-model-architecture}
\end{figure}

In the decoder part, they use lateral connections from different parts of the encoder, which enables the model to keep high-resolution information. They apply 1 x 1 convolution to generate a fixed number of channels for every lateral connection.
The decoder network has a feature pyramid structure involving four modules. The output from the preceding level is fed into a 3 x 3 transposed convolution at each level of the pyramid, followed by a bilinear upsampling to produce a fixed number of higher-resolution features.
The features from the corresponding lateral connection are concatenated with the features from the upsampled samples. ReLU and batch normalization are used like in the encoder, to obtain the module's final output.
As depicted in Figure \ref{fig:nimbro-model-architecture}, the model predicts heatmaps of both keypoints and limbs for scale 1/4 and only
heatmaps of keypoints for scale 1/2, where each scale is supervised with an intermediate loss \parencite{amini2021}.
For evaluation metrics, they use Object Keypoint Similarity (OKS) metric from COCO keypoint dataset \parencite{ronchi2017}.
The OKS metric is robust to the number of visible keypoints since it gives equal weight to robot instances with various quantities of visible keypoints.
The evaluation metrics they used are as follows: AP (the mean average precision over 10 OKS thresholds = [0.50:0.05:0.95]), AP50
(AP at OKS threshold = 0.50), AP75, APM for medium scale robot instances, APL for large scale instances, and AR (the mean of average recall over 10 OKS thresholds) \parencite{amini2021}.

\subsection{Keypoint RCNN}
\label{subsec:rcnn}

The architecture of Keypoint RCNN resembles the Mask-RCNN. They just differ in the output size, and the way the keypoints are encoded in the keypoint mask. Mask R-CNN itself is extended on the basis of Faster R-CNN,
in addition to the shared backbone network and FPN (Feature Pyramid Networks), it in parallel adds a new branch for predicting the object mask on the bounding box recognition branch.
The network can also be simply expanded to carry out additional tasks, including Keypoint RCNN for Person Keypoint Detection. The keypoint head of Keypoint R-CNN is very similar to the mask head of Mask R-CNN. The mask head obtains an output resolution of
28 x 28 through four 3 x 3 convolutional layers with a channel of 256 which is followed by an up-sampling layer, and finally
employ a convolutional layer to make its dimensions consistent with the object category. On the other hand, the keypoint head gets an output resolution of 56 x 56 through eight 3 x 3 convolutional
layers with 512 channels and followed by two upsampling layers \parencite{zhang2021}.
The full Keypoint RCNN architecure can be seen in Figure \ref{fig:keypoint-rcnn-architecture}.

\begin{figure}[ht]
  \centering
  \includegraphics[scale=0.9]{gambar/keypoint-rcnn-arch.png}
  \caption{Keypoint RCNN Architecture}
  \label{fig:keypoint-rcnn-architecture}
\end{figure}